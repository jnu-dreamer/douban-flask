import re # 用于正则匹配
import time # 用于延迟
import urllib.error # 用于处理URL请求异常
import urllib.request # 用于处理URL请求
from urllib.parse import quote # URL编码
from typing import Dict, List, Optional # 用于类型提示
from bs4 import BeautifulSoup # 用于解析HTML


class DoubanSpider:
    """豆瓣电影列表爬虫 (支持 Top250 或指定标签)."""
    def __init__(self, base_url: str = "https://movie.douban.com/top250", tag: str = "", pages: int = 10, limit: int = 200, delay: float = 1.0):
        self.base_url = base_url.rstrip("/")
        self.tag = tag
        if self.tag:
            # base_url 用作HTTP请求头的Referer，API请求用作URL
            self.base_url = f"https://movie.douban.com/tag/{quote(self.tag)}"
        
        self.pages = pages
        self.limit = limit
        self.delay = delay
        self.headers = { # 设置HTTP头，伪装为Chrome浏览器
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        }
        # 例如: "123456 人评价"
        self.rating_count_pattern = re.compile(r"(\d+)\s*人评价") # 匹配评价人数
        # 例如: "1994 / 美国 / 剧情 犯罪" (Top250 标准格式)
        self.meta_pattern = re.compile(r"(\d{4})\s*/\s*([^/]+)\s*/\s*(.+)") # 匹配年份、国家、类型

    def fetch(self, progress_callback=None, save_callback=None) -> List[Dict[str, str]]:
        """抓取豆瓣电影列表，返回电影字典列表
        Args:
            progress_callback: 进度回调函数
            save_callback: 数据保存回调函数 (batch_data -> None)
        """
        records: List[Dict[str, str]] = [] 
        
        # 模式一：标签搜索 (API 一次性请求)
        if self.tag:
            url = f"https://movie.douban.com/j/search_subjects?type=movie&tag={quote(self.tag)}&sort=recommend&page_limit={self.limit}&page_start=0"
            self.headers.update({"Referer": f"https://movie.douban.com/tag/{quote(self.tag)}"})
            
            print(f"Fetching API {url} ...")
            if progress_callback:
                progress_callback(1, 1)

            content = self._get(url)
            if content:
                batch = self._parse_json(content)
                if save_callback:
                    print(f"  > Saving {len(batch)} records...")
                    save_callback(batch)
                records.extend(batch)
                
        # 模式二：Top 250 (网页分页抓取)
        else:
            for i in range(self.pages):  # 循环，从第0页到第self.pages-1页
                if progress_callback:
                    progress_callback(i + 1, self.pages) # 更新进度
                
                start = i * 25
                url = f"{self.base_url}?start={start}"
                self.headers.update({"Referer": "https://movie.douban.com/top250"})

                print(f"Fetching {url} ...")
                content = self._get(url)
                if not content:
                    continue
                
                batch = self._parse(content)
                if save_callback:
                     print(f"  > Saving {len(batch)} records...")
                     save_callback(batch)
                records.extend(batch)
                    
                if self.delay:
                    time.sleep(self.delay)
        return records

    def _get(self, url: str) -> str:
        request = urllib.request.Request(url, headers=self.headers)
        try:
            # 设置超时时间为 5 秒，防止长时间卡死
            with urllib.request.urlopen(request, timeout=5) as resp:
                return resp.read().decode("utf-8")
        except Exception as e:
            # 捕获超时或其他错误
            print(f"Request Error for {url}: {e}")
            return ""

    def _parse_json(self, json_str: str) -> List[Dict[str, str]]:
        import json
        records: List[Dict[str, str]] = []
        try:
            data = json.loads(json_str)
            subjects = data.get("subjects", [])
            for sub in subjects:
                # API 返回有限字段: rate, title, url, cover, is_new, id
                # 我们将其映射到我们的 schema
                
                info_link = sub.get("url", "")
                
                # 获取详情以补充缺失字段
                print(f"  > Fetching details for {sub.get('title', '')} ...")
                details = self._get_movie_details(info_link)

                records.append({
                    "info_link": info_link,
                    "pic_link": sub.get("cover", ""),
                    "cname": sub.get("title", ""),
                    "score": sub.get("rate", "0"),
                    "rated": details["rated"], 
                    "introduction": details["introduction"],
                    "year_release": sub.get("year") or details["year"], 
                    "country": details["country"], 
                    "category": details["category"] if details["category"] else self.tag, 
                    "directors": details["directors"],
                    "actors": details["actors"],
                })
        except json.JSONDecodeError:
            print("Failed to decode JSON response")
        return records

    def _get_movie_details(self, url: str) -> Dict[str, str]:
        """抓取电影详情页，获取完整信息"""
        details = {
            "introduction": "", "country": "", "year": "", "category": "",
            "rated": "", "directors": "", "actors": ""
        }
        if not url:
            return details
            
        try:
            time.sleep(0.5) # 缩短延迟到 0.5s，并在下方 _get 增加了超时控制
            html = self._get(url)
            if not html:
                return details
                
            soup = BeautifulSoup(html, "html.parser")
            
            # 1. 简介 (v:summary)
            related_info = soup.find("div", class_="related-info")
            if related_info:
                span = related_info.find("span", property="v:summary")
                if span:
                    # 处理可能存在的 <br>
                    details["introduction"] = span.get_text(strip=True)

            # 2. Meta 信息 (#info)
            info_div = soup.find("div", id="info")
            if info_div:
                text = info_div.get_text()
                # 制片国家/地区
                if "制片国家/地区:" in text:
                    parts = text.split("制片国家/地区:")
                    if len(parts) > 1:
                        details["country"] = parts[1].split("\n")[0].strip()
                
                # 上映年份 (优先从 JSON 或 List 获取，这里作为补充)
                if "上映日期:" in text or "首播:" in text:
                    y_parts = re.findall(r"(\d{4})", text)
                    if y_parts:
                        details["year"] = y_parts[0]

            # 3. 评价人数 (v:votes)
            vote_tag = soup.find("span", property="v:votes")
            if vote_tag:
                 details["rated"] = vote_tag.get_text(strip=True)

            # 4. 类型 (v:genre) -  获取完整类型列表
            genre_tags = soup.find_all("span", property="v:genre")
            if genre_tags:
                details["category"] = " ".join([t.get_text(strip=True) for t in genre_tags])

            # 5. 导演 (v:directedBy)
            bus = soup.find_all("a", rel="v:directedBy")
            if bus:
                details["directors"] = " ".join([b.get_text(strip=True) for b in bus])
            
            # 6. 主演 (v:starring)
            acts = soup.find_all("a", rel="v:starring")
            if acts:
                details["actors"] = " ".join([a.get_text(strip=True) for a in acts[:5]]) # 仅取前5

        except Exception as e:
            print(f"Failed to fetch details for {url}: {e}")
            
        return details

    def _parse(self, html: str) -> List[Dict[str, str]]: # 解析HTML（top250）
        soup = BeautifulSoup(html, "html.parser") 
        records: List[Dict[str, str]] = []
        
        items = soup.find_all("div", class_="item")
        for item in items:
            link_tag = item.find("a", href=True)
            info_link = link_tag["href"] if link_tag else ""

            img_tag = item.find("img")
            pic_link = img_tag["src"] if img_tag else ""

            title_tag = item.find("span", class_="title")
            cname = title_tag.get_text(strip=True) if title_tag else ""

            rating_tag = item.find("span", class_="rating_num")
            score = rating_tag.get_text(strip=True) if rating_tag else ""

            # 暂时获取列表页的基础信息作为兜底
            list_rated = ""
            star_div = item.find("div", class_="star")
            if star_div:
                m = self.rating_count_pattern.search(star_div.get_text())
                if m:
                    list_rated = m.group(1)

            # 列表页的简介通常只是短评(quote)，不是真正的简介
            quote_tag = item.find("span", class_="inq")
            short_quote = quote_tag.get_text(strip=True) if quote_tag else ""

            # 解析列表页的元数据 (年份/国家/类型)
            list_year = list_country = list_category = ""
            list_directors = list_actors = ""
            
            bd_div = item.find("div", class_="bd")
            if bd_div:
                p_tag = bd_div.find("p")
                if p_tag:
                    raw_text = str(p_tag)
                    parts = re.split(r"<br\s*/?>", raw_text)
                    if len(parts) >= 1:
                        line1 = BeautifulSoup(parts[0], "html.parser").get_text(strip=True)
                        if "导演:" in line1:
                            d_parts = line1.split("主演:")
                            list_directors = d_parts[0].replace("导演:", "").strip()
                            if len(d_parts) > 1:
                                list_actors = d_parts[1].strip()
                    if len(parts) >= 2:
                        line2 = BeautifulSoup(parts[1], "html.parser").get_text(strip=True)
                        m = self.meta_pattern.search(line2)
                        if m:
                            list_year = m.group(1).strip()
                            list_country = m.group(2).strip()
                            list_category = m.group(3).strip()

            # --- 关键修改：进入详情页抓取完整信息 ---
            print(f"  > Fetching details for {cname} ...")
            details = self._get_movie_details(info_link)
            
            # 合并逻辑：优先使用详情页信息，列表页信息兜底
            final_record = {
                "info_link": info_link,
                "pic_link": pic_link,
                "cname": cname,
                "score": score,
                "rated": details["rated"] if details["rated"] else list_rated,
                # 简介：如果有详情页简介则用详情页，否则用 Quote，实在没有为空
                "introduction": details["introduction"] if details["introduction"] else short_quote,
                "year_release": details["year"] if details["year"] else list_year,
                "country": details["country"] if details["country"] else list_country,
                "category": details["category"] if details["category"] else list_category,
                "directors": details["directors"] if details["directors"] else list_directors,
                "actors": details["actors"] if details["actors"] else list_actors,
            }

            records.append(final_record)

        return records


__all__ = ["DoubanSpider"]
