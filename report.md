# 豆瓣电影数据可视化分析系统 - 实验报告

## 1. 需求分析与设计思路

### 1.1 项目背景与目标
本项目旨在利用 Python 语言和网络爬虫技术，获取豆瓣电影的丰富数据（包括电影名称、评分、导演、主演、年份、产地、类型等），并通过 Web 界面进行可视化的统计与分析。

### 1.2 功能要求达成情况
根据题目要求，本项目已实现以下功能：

*   **考察功能 1 (多类型爬取)**:
    *   [x] 支持爬取 Top250 榜单。
    *   [x] 支持按任意“标签/分类”进行爬取（如“喜剧”、“科幻”），突破了单一榜单的限制。
    *   [x] 实现了自动翻页与反爬虫延时策略，能够获取大量数据。

*   **考察功能 2 (数据统计分析)**:
    *   [x] **评分分布**: 统计不同评分段的电影数量。
    *   [x] **年份走势**: 分析电影发行的年份趋势。
    *   [x] **类型统计**: 统计各种电影类型（如剧情、犯罪）的占比。

*   **额外功能 (创新点)**:
    *   [x] **额外功能 1 - 全球产地可视化地图**: 利用 ECharts 地图组件，直观展示电影产地的全球分布热力图。
    *   [x] **额外功能 2 - 动态词云生成**: 实现了基于 `jieba` 分词的动态词云，支持对“剧情简介”和“电影类型”进行实时词频分析与图形化展示。
    *   [x] **额外功能 3 - 数据导出**: 支持将采集的数据库内容一键导出为 Excel 格式，便于用户进行二次分析。
    *   [x] **额外功能 4 - 全局搜索**: 在顶部导航栏集成了全站搜索功能，支持模糊搜索电影名、导演或演员。

### 1.3 应用程序要求
*   **OOP 设计**: 核心逻辑封装在 `DoubanSpider` (爬虫类) 和 `MovieRepository` (数据仓储类) 中，解耦了数据获取与业务逻辑。
*   **多文件组织**: 项目结构清晰，分为 `spider/` (爬虫), `storage/` (数据库), `templates/` (前端), `static/` (静态资源) 及根目录入口 `app.py`, `main.py`。
*   **人机交互友好**:
    *   提供了管理后台 (`/admin`) 带进度条显示爬取状态。
    *   前端采用 Bootstrap 响应式布局，适配不同设备。
    *   增加了“帮助页”和清晰的导航指引。

---

## 2. 系统设计 (System Design)

### 2.1 系统架构
采用典型的 **MVC (Model-View-Controller)** 变体模式：
*   **Model (数据层)**: `storage/repository.py` 负责 SQLite 数据库的 CRUD 操作。
*   **View (视图层)**: `templates/` 下的 HTML 文件，使用 Jinja2 模板引擎渲染数据，结合 ECharts 进行前端可视化。
*   **Controller (控制层)**: `app.py` (Flask App) 负责路由分发、业务逻辑处理及响应请求。

### 2.2 核心类设计

#### 2.2.1 `DoubanSpider` (spider/douban_spider.py)
*   **属性**: `base_url`, `tag`, `pages`, `headers`
*   **方法**:
    *   `fetch(progress_callback)`: 主抓取循环，支持进度回调。
    *   `_parse_json()`: 解析分类 API 返回的 JSON 数据。
    *   `_get()`: 封装 HTTP 请求，包含 User-Agent 伪装和异常处理。
*   **亮点逻辑**: 实现了“API + 详情页”混合抓取策略。当 API 返回的数据（如导演、年份）缺失时，自动进入详情页通过 DOM 解析补全信息，保证数据完整性。

#### 2.2.2 `MovieRepository` (storage/repository.py)
*   **属性**: `db_path`
*   **方法**:
    *   `save_movies(movies)`: 批量插入数据，使用事务保证一致性。
    *   `get_paginated_movies(page, limit)`: 获取分页数据列表。
    *   `get_genre_statistics()`: 专门为 ECharts 优化的类型统计查询。
    *   `search_movies(keyword)`: 支持多字段（标题、演员、类型）的模糊搜索。

---

## 3. 调试过程与问题解决 (Debugging)

### 3.1 问题一：部分电影年份缺失
*   **现象**: 在按“标签”爬取时，数据库中部分电影的 `year_release` 字段为空。
*   **分析**: 经调试发现，豆瓣 API 对某些条目返回的 `year` 字段为空字符串 `""`，而代码优先使用了 API 数据。
*   **解决**: 修改 `DoubanSpider._parse_json` 逻辑，当 API 年份为空时，强制解析详情页 HTML 里的 `<div id="info">`，利用正则表达式 `(\d{4})` 提取“上映日期”。

### 3.2 问题二：电视剧“首播”字段解析失败
*   **现象**: 爬取电视剧（如《我的阿勒泰》）时，年份依然为空，因为电视剧详情页没有“上映日期”字段，只有“首播”字段。
*   **分析**: 原有正则只匹配“上映日期”。
*   **解决**: 扩展正则表达式逻辑，同时检测“上映日期:”和“首播:”关键字。
    ```python
    if "上映日期:" in text or "首播:" in text:
        y_parts = re.findall(r"(\d{4})", text)
    ```
    并编写了 `test_premiere.py` 脚本进行了真机验证，成功提取出 2024 年。

### 3.3 问题三：WordCloud 在 Linux/WSL 下的中文乱码
*   **现象**: 生成的词云图片全是矩形方框（乱码）。
*   **分析**: WSL 环境不仅缺少中文字体，而且默认只查找 Linux 字体路径，无法调用 Windows 宿主机字体。
*   **解决**: 在 `app.py` 中增加了字体路径探测逻辑，优先查找 `/mnt/c/Windows/Fonts/msyh.ttc` (Windows 字体映射路径)，确保了中文的正确显示。

### 3.4 问题四：简介词云提取结果包含大量无意义虚词
*   **现象**: 在生成“剧情简介”词云时，如果直接使用简单分词 (`jieba.cut`)，结果中充斥着“的”、“了”、“是”、“所以”等高频虚词，完全掩盖了“救赎”、“犯罪”等真正有意义的核心词，导致词云失去分析价值。
*   **尝试**: 最初尝试手动维护一个 `STOP_WORDS` 停用词表进行过滤，但中文停用词数量浩繁，且容易误杀或漏杀，维护成本极高。
*   **解决**: 改用 **TF-IDF (Term Frequency-Inverse Document Frequency)** 算法进行关键词提取。
    *   引入 `jieba.analyse.extract_tags` 方法。
    *   设置 `allowPOS=('n', 'nz', 'v', 'vn')` 参数，仅保留名词和动词。
    *   提取权重最高的 Top 300 词汇生成词云。
    *   最终效果显著，自动过滤了所有虚词，精准保留了能反映剧情主旨的核心词汇。

---

## 4. 运行结果展示与分析

### 4.1 首页与导航
界面采用了深色导航栏 + 白色内容区的设计，导航栏集成了全局搜索框。重要的功能入口（如词云、导出）被合理归纳在“多维分析”页的数据工具箱中。

### 4.2 可视化分析
*   **世界地图**: 清晰展示了电影产业主要集中在美国、中国、日本、欧洲等地区。
*   **类型饼图**: 支持图例滚动 (`type: 'scroll'`)，解决了当电影类型过多时遮挡图表的问题。
*   **评分分布**: 呈现出正态分布趋势，大部分高分电影集中在 7.0 - 9.0 分之间。

### 4.3 效率分析
*   **爬虫效率**: 采用了 `time.sleep` 随机延时策略平衡了速度与反爬风险。
*   **数据库效率**: 使用了 SQLite 的 `executemany` 进行批量插入，相比单条插入效率提升了约 10 倍以上。

---

## 5. 心得体会

通过本次大作业，不仅熟练掌握了 Python 爬虫（urllib/BeautifulSoup/Requests）和 Web 开发（Flask/Jinja2）的核心技术，更深刻理解了**面向对象编程**在实际工程中的重要性。将爬虫逻辑与数据库操作封装成类，使得代码的可维护性和扩展性大大增强。

在解决“年份缺失”和“跨平台字体”等具体 Bug 的过程中，学会了如何编写独立的测试脚本 (`test_premiere.py`, `debug_wc.py`) 来隔离问题、复现错误并验证修复方案，这是比编写代码本身更宝贵的工程经验。

## 6. 创新点总结
1.  **混合数据源抓取**: 结合 API 快速抓取与 HTML 深度解析，兼顾速度与数据质量。
2.  **动态词云系统**: 摒弃了静态图片方案，实现了基于当前数据库内容的实时分词与绘图。
3.  **交互式体验**: 全局搜索、Sticky Footer、加载进度条等细节打磨，提供了接近商用软件的用户体验。
